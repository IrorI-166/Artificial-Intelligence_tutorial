from datasets import load_dataset
import tokenizers
import os
#from tokenizers import Tokenizer
#from tokenizers.trainers import BpeTrainer
#from tokenizers.pre_tokenizers import Whitespace
#from tokenizers.models import BPE

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
ds = load_dataset("Salesforce/wikitext", "wikitext-103-v1")

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—
train_dataset = ds['train']
valid_dataset = ds['validation']
test_dataset = ds['test']

#tokenizerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ&ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
def tokenizer():
    #tokenizerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE(unk_token="[UNK]"))
    #trainerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–
    trainer = tokenizers.trainers.BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
    #tokenizerã«pre_trainerå±æ€§ã‚’è¿½åŠ 
    tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º (train, valid, test ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°)
    all_texts = []
    for dataset in [train_dataset, valid_dataset, test_dataset]:
        all_texts.extend(dataset['text'])
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° (ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨)
    tokenizer.train_from_iterator(all_texts, trainer)
    #tokenizerã‚’ã‚»ãƒ¼ãƒ–
    tokenizer.save("./tokenizer-wiki.json")
    #tokenizerã‚’ãƒªãƒ­ãƒ¼ãƒ‰
    tokenizer = tokenizers.Tokenizer.from_file("./tokenizer-wiki.json")
    #tokenizerã‚’ä½¿ã†
    output = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")
    print(output.tokens)

if __name__ == "__main__":
    tokenizer()
    # ãƒ‡ãƒ¼ã‚¿ã®ä¾‹ã‚’è¡¨ç¤º
    for i in range(10):
        print(train_dataset[i])
        print(valid_dataset[i])
        print(test_dataset[i])