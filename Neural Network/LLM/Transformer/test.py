import tokenizers
import numpy as np

#tokenizerã‚’ãƒªãƒ­ãƒ¼ãƒ‰
tokenizer = tokenizers.Tokenizer.from_file("Neural Network/LLM/Transformer/tokenizer-wiki.json")

#tokenizerã‚’ä½¿ã†
batch_sentences = [
"But what about second breakfast?",
"Don't think he knows about second breakfast, Pip.",
"What about elevensies?",
]
output1 = tokenizer.encode("ã“ã‚“ã«ã¡ã¯ã€æ°—åˆ†ã¯ã©ã†ï¼Ÿ")
output2 = tokenizer.encode("Hello, y'all!", "How are you ğŸ˜ ?")
print(output1.tokens)
print(output2.tokens)
print(output2.attention_mask)
print(output2.type_ids)
#output2ã‚’numpyé…åˆ—ã«å¤‰æ›ã—ã¦çµåˆã™ã‚‹
# tokensã‚’NumPyé…åˆ—ã«å¤‰æ›ï¼ˆæ–‡å­—åˆ—ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰
tokens_np = np.array(output2.tokens)
# attention_maskã‚’NumPyé…åˆ—ã«å¤‰æ›
attention_mask_np = np.array(output2.attention_mask)
# type_idsã‚’NumPyé…åˆ—ã«å¤‰æ›
type_ids_np = np.array(output2.type_ids)

# 2æ¬¡å…ƒé…åˆ—ã«çµåˆï¼ˆvstackã§ç¸¦æ–¹å‘ã«çµåˆï¼‰
combined_np_array = np.vstack([tokens_np, attention_mask_np, type_ids_np])

# çµæœã‚’è¡¨ç¤º
print("Combined 2D NumPy Array:")
print(combined_np_array)

batch_output = tokenizer.encode_batch(
    [["Hello, y'all!", "How are you ğŸ˜ ?"],
    ["Hello to you too!", "I'm fine, thank you!"]]
    )
print("------batch_out------")
for i in range(2):
    print(batch_output[i].tokens)
    print(batch_output[i].ids)
    print(batch_output[i].type_ids)
    print(batch_output[i].attention_mask)