from datasets import load_dataset
import tokenizers
import numpy as np
#from tokenizers import Tokenizer
#from tokenizers.trainers import BpeTrainer
#from tokenizers.pre_tokenizers import Whitespace
#from tokenizers.models import BPE
#from tokenizers.processors import TemplateProcessing

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
ds = load_dataset("Salesforce/wikitext", "wikitext-103-v1")

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—
train_dataset = ds['train']
valid_dataset = ds['validation']
test_dataset = ds['test']

#tokenizerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ&ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
def tokenizer():
    #tokenizerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE(unk_token="[UNK]"))
    #trainerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–
    trainer = tokenizers.trainers.BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
    #tokenizerã«pre_trainerå±æ€§ã‚’è¿½åŠ 
    tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º (train, valid, test ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°)
    all_texts = []
    for dataset in [train_dataset, valid_dataset, test_dataset]:
        all_texts.extend(dataset['text'])
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° (ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨)
    tokenizer.train_from_iterator(all_texts, trainer)
    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®IDã‚’æ˜ç¤ºçš„ã«å–å¾—
    cls_token_id = tokenizer.token_to_id("[CLS]")
    sep_token_id = tokenizer.token_to_id("[SEP]")
    #torkenizerã«post_processå±æ€§ã‚’è¿½åŠ 
    tokenizer.post_processor = tokenizers.processors.TemplateProcessing(
        single="[CLS] $A [SEP]",
        pair="[CLS] $A [SEP] $B:1 [SEP]:1",
        special_tokens=[
            ("[CLS]", cls_token_id),
            ("[SEP]", sep_token_id),
        ],
    )
    #tokenizerã«enable_paddingå±æ€§ã‚’è¿½åŠ 
    tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")
    #tokenizerã‚’ã‚»ãƒ¼ãƒ–
    tokenizer.save("Neural Network/LLM/Transformer/tokenizer-wiki.json")

def loadTokenizer():
    #tokenizerã‚’ãƒªãƒ­ãƒ¼ãƒ‰
    tokenizer = tokenizers.Tokenizer.from_file("Neural Network/LLM/Transformer/tokenizer-wiki.json")
    return tokenizer

def useTokenizer(tokenizer):
    #tokenizerã‚’ä½¿ã†
    batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
    ]
    output1 = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")
    output2 = tokenizer.encode("Hello, y'all!", "How are you ğŸ˜ ?")
    print(output1.tokens)
    print(output2.tokens)
    print(output2.attention_mask)
    print(output2.type_ids)
    #output2ã‚’numpyé…åˆ—ã«å¤‰æ›ã—ã¦çµåˆã™ã‚‹
    # tokensã‚’NumPyé…åˆ—ã«å¤‰æ›ï¼ˆæ–‡å­—åˆ—ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰
    tokens_np = np.array(output2.tokens)
    # attention_maskã‚’NumPyé…åˆ—ã«å¤‰æ›
    attention_mask_np = np.array(output2.attention_mask)
    # type_idsã‚’NumPyé…åˆ—ã«å¤‰æ›
    type_ids_np = np.array(output2.type_ids)

    # 2æ¬¡å…ƒé…åˆ—ã«çµåˆï¼ˆvstackã§ç¸¦æ–¹å‘ã«çµåˆï¼‰
    combined_np_array = np.vstack([tokens_np, attention_mask_np, type_ids_np])

    # çµæœã‚’è¡¨ç¤º
    print("Combined 2D NumPy Array:")
    print(combined_np_array)

    batch_output = tokenizer.encode_batch(
        [["Hello, y'all!", "How are you ğŸ˜ ?"],
        ["Hello to you too!", "I'm fine, thank you!"]]
        )
    print("------batch_out------")
    for i in range(2):
        print(batch_output[i].tokens)
        print(batch_output[i].ids)
        print(batch_output[i].type_ids)
        print(batch_output[i].attention_mask)

def createTokenEmbeddingMatrix(tokenizer):
    #ãƒˆãƒ¼ã‚¯ãƒ³IDã®ç·æ•°
    vocab_size = tokenizer.get_vocab_size()
    #åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ(ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼)
    embedding_dim = 768
    # åŸ‹ã‚è¾¼ã¿è¡Œåˆ—ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ– ({ãƒˆãƒ¼ã‚¯ãƒ³IDã®æ•°}è¡Œ{åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°}åˆ—ã®è¡Œåˆ—)
    embedding_matrix = np.random.randn(vocab_size, embedding_dim)

    return embedding_matrix

def useTokenEmbeddingMatrix(embedding_matrix):
    # ãƒˆãƒ¼ã‚¯ãƒ³IDãƒªã‚¹ãƒˆï¼ˆä¾‹: [101, 7592, 2026]ï¼‰
    input_tokens = [101, 7592, 2026]

    # ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¯¾å¿œã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    embedded_tokens = [embedding_matrix[token_id] for token_id in input_tokens]

    # çµæœã®è¡¨ç¤º
    for i, token_id in enumerate(input_tokens):
        print(f"Token ID: {token_id}, Embedding Vector: {embedded_tokens[i][:5]}...")  # ãƒ™ã‚¯ãƒˆãƒ«ã®æœ€åˆã®5ã¤ã®è¦ç´ ã‚’è¡¨ç¤º

def createPositionalEmbeddingMatrix(tokenizer):
    #ãƒˆãƒ¼ã‚¯ãƒ³IDã®ç·æ•°
    vocab_size = tokenizer.get_vocab_size()
    #åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ(ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼)
    embedding_dim = 768
    #ä½ç½®ç¬¦å·è¡Œåˆ—ã‚’åˆæœŸåŒ–
    positional_matrix = np.empty((vocab_size, embedding_dim))

    #ä½ç½®ç¬¦å·è¡Œåˆ—ã‚’ãƒ«ãƒ¼ãƒ—ã—ã¦ç¬¦å·ã‚’è¿½åŠ 
    #å˜èªä½ç½®ã§ãƒ«ãƒ¼ãƒ—(åˆ—ã®æ±ºå®š)
    for i in range(vocab_size):
        #åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã§ãƒ«ãƒ¼ãƒ—(è¡Œã®æ±ºå®š)
        for k in range(embedding_dim//2): #åˆ‡ã‚Šæ¨ã¦é™¤ç®—ã§ä½ç½®ã‚’æ±ºå®š
            t = i / (10000 ** (2 * k / embedding_dim))
            positional_matrix[i, 2 * k] = np.sin(t)
            positional_matrix[i, 2 * k + 1] = np.cos(t)

    return positional_matrix

def combineTokenAndPositional(embedding_matrix, positional_matrix):
    embedding_matrix += positional_matrix
    return embedding_matrix

def softmax(x):
    ex = np.exp(x - np.max(x))  # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã‚’é˜²ããŸã‚ã«æœ€å¤§å€¤ã‚’å¼•ã
    return ex / (np.sum(ex, axis=-1, keepdims=True) + 1e-10)  #0é™¤ç®—é˜²æ­¢ã«å°ã•ãªå€¤ã‚’åŠ ãˆã‚‹

# æ—¢å­˜ã®åŸ‹ã‚è¾¼ã¿è¡Œåˆ—ã¨é‡ã¿è¡Œåˆ—ã‚’å…ƒã«Q, K, Vã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã‚’å®šç¾©
def calculate_QKV(embedding_matrix, W_Q, W_K, W_V):
    """
    åŸ‹ã‚è¾¼ã¿è¡Œåˆ—ã«åŸºã¥ã„ã¦Query, Key, Valueã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
    :param embedding_matrix: åŸ‹ã‚è¾¼ã¿è¡Œåˆ— (ãƒˆãƒ¼ã‚¯ãƒ³æ•° Ã— åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ)
    :param W_Q: Queryã®é‡ã¿è¡Œåˆ—
    :param W_K: Keyã®é‡ã¿è¡Œåˆ—
    :param W_V: Valueã®é‡ã¿è¡Œåˆ—
    :return: Q, K, Vè¡Œåˆ—
    """
    # Queryè¡Œåˆ—ã‚’è¨ˆç®—
    Q = np.dot(embedding_matrix, W_Q)  # ãƒˆãƒ¼ã‚¯ãƒ³æ•° Ã— åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
    # Keyè¡Œåˆ—ã‚’è¨ˆç®—
    K = np.dot(embedding_matrix, W_K)  # ãƒˆãƒ¼ã‚¯ãƒ³æ•° Ã— åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
    # Valueè¡Œåˆ—ã‚’è¨ˆç®—
    V = np.dot(embedding_matrix, W_V)  # ãƒˆãƒ¼ã‚¯ãƒ³æ•° Ã— åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
    return Q, K, V

def Single_Head_Attention(Q, K, V, embedding_dim):
    """
    Scaled Dot-Product Attentionã®è¨ˆç®—
    :param Q: Queryè¡Œåˆ—(å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•° Ã— åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°ã®å½¢çŠ¶ã‚’æŒã¤è¡Œåˆ—)
    :param K: Keyè¡Œåˆ—
    :param V: Valueè¡Œåˆ—
    :return: Attentionã«ã‚ˆã‚‹å‡ºåŠ›
    """
    #åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ(ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼)
    embedding_dim = embedding_dim

    """
    ãƒ‰ãƒƒãƒˆç© QKt: Queryã¨Keyã®é¡ä¼¼æ€§ã‚’è¨ˆç®—ã—ã€å„ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é–¢é€£åº¦ã‚’å¾—ã‚‹ã€‚
    Queryã¨Keyã®è»¢ç½®è¡Œåˆ—ã®ãƒ‰ãƒƒãƒˆç©ã‚’è¨ˆç®—ã—ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    Qã®iè¡Œç›®ã¯å…¥åŠ›ã®iç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³(1è¡Œ768åˆ—ã®è¡Œåˆ—)â‡’Kã¨ã®ãƒ‰ãƒƒãƒˆç©ã§ä¸€ã¤ã®å€¤ãŒå‡ºã‚‹
    scoresãŒæŒã¤ã®ã¯å…¥åŠ›é•·Ã—å…¥åŠ›é•·ã‚µã‚¤ã‚ºã‚’æŒã¤è¡Œåˆ—ã¨ãªã‚‹
    """
    scores = np.dot(Q, K.T) / np.sqrt(embedding_dim)
    #scores = np.clip(scores, -500, 500)  # ã‚¹ã‚³ã‚¢ã‚’é©åº¦ã«ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°

    """
    ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’é©ç”¨ã—ã¦Attentionã®é‡ã¿ã‚’è¨ˆç®—
    å…¥åŠ›é•·Ã—å…¥åŠ›é•·ã‚µã‚¤ã‚ºã®ç¢ºç‡å€¤è¡Œåˆ—ãŒå¾—ã‚‰ã‚Œã‚‹
    ã“ã®ç¢ºç‡å€¤è¡Œåˆ—ã«ãŠã„ã¦iè¡Œjåˆ—ã®å€¤ã¯ã€
    å…¥åŠ›ä¸Šã®jç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒå…¥åŠ›ä¸Šã®iç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦ã©ã‚Œã ã‘æ³¨æ„ã‚’å‘ã‘ã‚‹ã‹ã‚’ç¤ºã™
    """
    attention_weights = softmax(scores)
    #attention_weights = np.clip(attention_weights, 1e-5, 1) # æ¥µç«¯ãªå°ã•ãªå€¤ã‚’é˜²ã
    """
    Attentioné‡ã¿ã¨Valueè¡Œåˆ—ã®ãƒ‰ãƒƒãƒˆç©ã‚’è¨ˆç®—ã—ã¦æœ€çµ‚çš„ãªå‡ºåŠ›ã‚’å¾—ã‚‹
    Vã¯å…¥åŠ›é•·Ã—åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã‚µã‚¤ã‚ºã®è¡Œåˆ—ã§å„ãƒˆãƒ¼ã‚¯ãƒ³ã®ã€Œæ„å‘³ã€ã‚’æŒã¡ã€
    Attentioné‡ã¿ã¯å…¥åŠ›é•·Ã—å…¥åŠ›é•·ã‚µã‚¤ã‚ºã®è¡Œåˆ—ã§ãã‚Œãã‚Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã™ã‚‹æ³¨æ„é‡ã‚’æŒã¤
    ãƒ‰ãƒƒãƒˆç©ã§Vã®1è¡Œã¨Attentioné‡ã¿ã®1åˆ—ã®é‡ã¿ä»˜ãå’ŒãŒå¾—ã‚‰ã‚Œã‚‹
    ã—ãŸãŒã£ã¦ã€ãƒ‰ãƒƒãƒˆç©ã®çµæœ1è¡ŒÃ—åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã‚µã‚¤ã‚ºã®è¡Œåˆ—ãŒå„ãƒˆãƒ¼ã‚¯ãƒ³ã«ã¤ã„ã¦å¾—ã‚‰ã‚Œã‚‹(å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°Ã—åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°è¡Œåˆ—)
    """
    output = np.dot(attention_weights, V)

    return output, attention_weights

def Multi_Head_Attention(Q, K, V, embedding_dim, num_heads):
    """
    MHAã¨ã¯åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒÃ·ãƒ˜ãƒƒãƒ‰æ•°ã®æ¬¡å…ƒã‚’ã‚‚ã£ã¦å…¥åŠ›ã‚’å¤šè§’çš„ã«è§£é‡ˆã—ã€
    ãã‚Œã‚’æœ€å¾Œã«çµåˆã™ã‚‹ã“ã¨ã§å…ƒã®å…¥åŠ›ã¨åŒã˜ã‚µã‚¤ã‚ºã®è¡Œåˆ—ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã§ã€
    ãƒ˜ãƒƒãƒ‰æ•°ãŒ8ã§ã‚ã‚Œã°8å€‹åˆ†ã®è§£é‡ˆã‚’æŒã£ãŸåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒãŒä½œæˆã§ãã‚‹
    """
    #åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ(ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼)
    embedding_dim = 768
    #MHAã®ãƒ˜ãƒƒãƒ‰æ•°
    num_heads = num_heads
    """
    ãƒ˜ãƒƒãƒ‰æ•°ã§åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’å‰²ã‚‹ç†ç”±ï¼š
    1ï¼è¨ˆç®—ã®åŠ¹ç‡åŒ–ï¼šå„ãƒ˜ãƒƒãƒ‰ã”ã¨ã®è¨ˆç®—é‡ã‚’æ¸›ã‚‰ã™ã“ã¨ãŒå¯èƒ½
    2ï¼åˆ†æ•£ã—ãŸä¸¦åˆ—å‡¦ç†ã®å®Ÿç¾ï¼šå„ãƒ˜ãƒƒãƒ‰ã”ã¨ã«ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰å…¥åŠ›ã‚’è§£é‡ˆã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šç²¾åº¦ã®é«˜ã„æ¨è«–ã‚’å®Ÿç¾
    """
    #å„ãƒ˜ãƒƒãƒ‰ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
    d_k = embedding_dim // num_heads

    #å„ãƒ˜ãƒƒãƒ‰ã§å¾—ã‚‰ã‚ŒãŸé‡ã¿è¡Œåˆ—ã¨æ³¨æ„è¡Œåˆ—ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
    heads_output = []
    heads_weights = []

    for i in range(num_heads):
        # Query, Key, Valueã®é‡ã¿è¡Œåˆ—ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ï¼‰
        W_Q = np.random.randn(embedding_dim, d_k) * np.sqrt(2 / embedding_dim)
        W_K = np.random.randn(embedding_dim, d_k) * np.sqrt(2 / embedding_dim)
        W_V = np.random.randn(embedding_dim, d_k) * np.sqrt(2 / embedding_dim)

        Q_head = np.dot(Q, W_Q)
        K_head = np.dot(K, W_K)
        V_head = np.dot(V, W_V)

        # 3. Single-Head Attentionã‚’å®Ÿè¡Œ
        head_output, head_weights = Single_Head_Attention(Q_head, K_head, V_head, d_k)

        heads_output.append(head_output)
        heads_weights.append(head_weights)

    # 4. Concatenate heads
    concatenated_heads = np.concatenate(heads_output, axis=-1)

    # 5. æœ€çµ‚çš„ãªç·šå½¢å¤‰æ›ã‚’å®Ÿè¡Œï¼ˆãƒ˜ãƒƒãƒ‰ã®å‡ºåŠ›ã‚’çµ±åˆï¼‰
    W_O = np.random.randn(concatenated_heads.shape[-1], embedding_dim)
    final_output = np.dot(concatenated_heads, W_O)

    return final_output, heads_weights



if __name__ == "__main__":
    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã®ä¾‹
    E_len = 100  # å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
    embedding_dim = 768  # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ

    # ãƒ‡ãƒ¼ã‚¿ã®ä¾‹ã‚’è¡¨ç¤º
    #for i in range(10):
        #print(train_dataset[i])
        #print(valid_dataset[i])
        #print(test_dataset[i])

    #tokenizerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ&ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
    #tokenizer()
    tokenizer = loadTokenizer()
    #useTokenizer(tokenizer)
    embedding_matrix = createTokenEmbeddingMatrix(tokenizer)
    #print(embedding_matrix)
    positional_matrix = createPositionalEmbeddingMatrix(tokenizer)
    #print(positional_matrix)
    embedding_matrix = combineTokenAndPositional(embedding_matrix, positional_matrix)
    #print(embedding_matrix)
    #useTokenEmbeddingMatrix(embedding_matrix)

    # Query, Key, Valueã®é‡ã¿è¡Œåˆ—ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ï¼‰
    W_Q = np.random.randn(embedding_dim, embedding_dim) * np.sqrt(2 / embedding_dim)
    W_K = np.random.randn(embedding_dim, embedding_dim) * np.sqrt(2 / embedding_dim)
    W_V = np.random.randn(embedding_dim, embedding_dim) * np.sqrt(2 / embedding_dim)
    # Q, K, Vã®è¨ˆç®—
    Q, K, V = calculate_QKV(embedding_matrix, W_Q, W_K, W_V)
    output, attention_weights = Multi_Head_Attention(Q, K, V, embedding_dim, 8)
    print(output)
    print(attention_weights)