from tokenizers import Tokenizer
from datasets import load_dataset
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.models import BPE

files = [f"data/wikitext-103-raw/wiki.{split}.raw" for split in ["test", "train", "valid"]]
tokenizer.train(files, trainer)

tokenizer.save("./tokenizer-wiki.json")
tokenizer = Tokenizer.from_file("./tokenizer-wiki.json")
output = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")

#EntryToken
E = []

#ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”¨æ„
def createDatasets():
    ds = load_dataset("Salesforce/wikitext", "wikitext-103-v1")
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—
    train_dataset = ds['train']
    valid_dataset = ds['validation']
    test_dataset = ds['test']

#tokenizerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
def torkenizer():
    #tokenizerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
    #trainerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–
    trainer = BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
    #tokenizerã«pre_trainerå±æ€§ã‚’è¿½åŠ 
    tokenizer.pre_tokenizer = Whitespace()