from datasets import load_dataset
import tokenizers
import numpy as np
#from tokenizers import Tokenizer
#from tokenizers.trainers import BpeTrainer
#from tokenizers.pre_tokenizers import Whitespace
#from tokenizers.models import BPE
#from tokenizers.processors import TemplateProcessing

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰
ds = load_dataset("Salesforce/wikitext", "wikitext-103-v1")

# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°/æ¤œè¨¼/ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å–å¾—
train_dataset = ds['train']
valid_dataset = ds['validation']
test_dataset = ds['test']

#tokenizerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ&ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
def tokenizer():
    #tokenizerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
    tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE(unk_token="[UNK]"))
    #trainerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆæœŸåŒ–
    trainer = tokenizers.trainers.BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"])
    #tokenizerã«pre_trainerå±æ€§ã‚’è¿½åŠ 
    tokenizer.pre_tokenizer = tokenizers.pre_tokenizers.Whitespace()
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º (train, valid, test ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°)
    all_texts = []
    for dataset in [train_dataset, valid_dataset, test_dataset]:
        all_texts.extend(dataset['text'])
    # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° (ãƒ†ã‚­ã‚¹ãƒˆã®ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨)
    tokenizer.train_from_iterator(all_texts, trainer)
    # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã®IDã‚’æ˜ç¤ºçš„ã«å–å¾—
    cls_token_id = tokenizer.token_to_id("[CLS]")
    sep_token_id = tokenizer.token_to_id("[SEP]")
    #torkenizerã«post_processå±æ€§ã‚’è¿½åŠ 
    tokenizer.post_processor = tokenizers.processors.TemplateProcessing(
        single="[CLS] $A [SEP]",
        pair="[CLS] $A [SEP] $B:1 [SEP]:1",
        special_tokens=[
            ("[CLS]", cls_token_id),
            ("[SEP]", sep_token_id),
        ],
    )
    #tokenizerã«enable_paddingå±æ€§ã‚’è¿½åŠ 
    tokenizer.enable_padding(pad_id=3, pad_token="[PAD]")
    #tokenizerã‚’ã‚»ãƒ¼ãƒ–
    tokenizer.save("Neural Network/LLM/Transformer/tokenizer-wiki.json")

def loadTokenizer():
    #tokenizerã‚’ãƒªãƒ­ãƒ¼ãƒ‰
    tokenizer = tokenizers.Tokenizer.from_file("Neural Network/LLM/Transformer/tokenizer-wiki.json")
    return tokenizer

def useTokenizer(tokenizer):
    #tokenizerã‚’ä½¿ã†
    batch_sentences = [
    "But what about second breakfast?",
    "Don't think he knows about second breakfast, Pip.",
    "What about elevensies?",
    ]
    output1 = tokenizer.encode("Hello, y'all! How are you ğŸ˜ ?")
    output2 = tokenizer.encode("Hello, y'all!", "How are you ğŸ˜ ?")
    print(output1.tokens)
    print(output2.tokens)
    print(output2.attention_mask)
    print(output2.type_ids)
    #output2ã‚’numpyé…åˆ—ã«å¤‰æ›ã—ã¦çµåˆã™ã‚‹
    # tokensã‚’NumPyé…åˆ—ã«å¤‰æ›ï¼ˆæ–‡å­—åˆ—ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰
    tokens_np = np.array(output2.tokens)
    # attention_maskã‚’NumPyé…åˆ—ã«å¤‰æ›
    attention_mask_np = np.array(output2.attention_mask)
    # type_idsã‚’NumPyé…åˆ—ã«å¤‰æ›
    type_ids_np = np.array(output2.type_ids)

    # 2æ¬¡å…ƒé…åˆ—ã«çµåˆï¼ˆvstackã§ç¸¦æ–¹å‘ã«çµåˆï¼‰
    combined_np_array = np.vstack([tokens_np, attention_mask_np, type_ids_np])

    # çµæœã‚’è¡¨ç¤º
    print("Combined 2D NumPy Array:")
    print(combined_np_array)

    batch_output = tokenizer.encode_batch(
        [["Hello, y'all!", "How are you ğŸ˜ ?"],
        ["Hello to you too!", "I'm fine, thank you!"]]
        )
    print("------batch_out------")
    for i in range(2):
        print(batch_output[i].tokens)
        print(batch_output[i].ids)
        print(batch_output[i].type_ids)
        print(batch_output[i].attention_mask)

def createTokenEmbeddingMatrix(tokenizer):
    #ãƒˆãƒ¼ã‚¯ãƒ³IDã®ç·æ•°
    vocab_size = tokenizer.get_vocab_size()
    #åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ(ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼)
    embedding_dim = 768
    # åŸ‹ã‚è¾¼ã¿è¡Œåˆ—ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ– ({ãƒˆãƒ¼ã‚¯ãƒ³IDã®æ•°}è¡Œ{åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°}åˆ—ã®è¡Œåˆ—)
    embedding_matrix = np.random.randn(vocab_size, embedding_dim)

    return embedding_matrix

def useTokenEmbeddingMatrix(embedding_matrix):
    # ãƒˆãƒ¼ã‚¯ãƒ³IDãƒªã‚¹ãƒˆï¼ˆä¾‹: [101, 7592, 2026]ï¼‰
    input_tokens = [101, 7592, 2026]

    # ãƒˆãƒ¼ã‚¯ãƒ³IDã«å¯¾å¿œã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã‚’å–å¾—
    embedded_tokens = [embedding_matrix[token_id] for token_id in input_tokens]

    # çµæœã®è¡¨ç¤º
    for i, token_id in enumerate(input_tokens):
        print(f"Token ID: {token_id}, Embedding Vector: {embedded_tokens[i][:5]}...")  # ãƒ™ã‚¯ãƒˆãƒ«ã®æœ€åˆã®5ã¤ã®è¦ç´ ã‚’è¡¨ç¤º

def createPositionalEmbeddingMatrix(tokenizer):
    #ãƒˆãƒ¼ã‚¯ãƒ³IDã®ç·æ•°
    vocab_size = tokenizer.get_vocab_size()
    #åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ(ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼)
    embedding_dim = 768
    #ä½ç½®ç¬¦å·è¡Œåˆ—ã‚’åˆæœŸåŒ–
    positional_matrix = np.empty((vocab_size, embedding_dim))

    #ä½ç½®ç¬¦å·è¡Œåˆ—ã‚’ãƒ«ãƒ¼ãƒ—ã—ã¦ç¬¦å·ã‚’è¿½åŠ 
    #å˜èªä½ç½®ã§ãƒ«ãƒ¼ãƒ—(åˆ—ã®æ±ºå®š)
    for i in range(vocab_size):
        #åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã§ãƒ«ãƒ¼ãƒ—(è¡Œã®æ±ºå®š)
        for k in range(embedding_dim//2): #åˆ‡ã‚Šæ¨ã¦é™¤ç®—ã§ä½ç½®ã‚’æ±ºå®š
            t = i / (10000 ** (2 * k / embedding_dim))
            positional_matrix[i, 2 * k] = np.sin(t)
            positional_matrix[i, 2 * k + 1] = np.cos(t)

    return positional_matrix

def combineTokenAndPositional(embedding_matrix, positional_matrix):
    embedding_matrix += positional_matrix
    return embedding_matrix

def softmax(x):
    ex = np.exp(x - np.max(x))  # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã‚’é˜²ããŸã‚ã«æœ€å¤§å€¤ã‚’å¼•ã
    return ex / (np.sum(ex, axis=-1, keepdims=True) + 1e-10)  #0é™¤ç®—é˜²æ­¢ã«å°ã•ãªå€¤ã‚’åŠ ãˆã‚‹

# æ—¢å­˜ã®åŸ‹ã‚è¾¼ã¿è¡Œåˆ—ã¨é‡ã¿è¡Œåˆ—ã‚’å…ƒã«Q, K, Vã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°ã‚’å®šç¾©
def calculate_QKV(embedding_matrix, W_Q, W_K, W_V):
    """
    åŸ‹ã‚è¾¼ã¿è¡Œåˆ—ã«åŸºã¥ã„ã¦Query, Key, Valueã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
    :param embedding_matrix: åŸ‹ã‚è¾¼ã¿è¡Œåˆ— (ãƒˆãƒ¼ã‚¯ãƒ³æ•° Ã— åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ)
    :param W_Q: Queryã®é‡ã¿è¡Œåˆ—
    :param W_K: Keyã®é‡ã¿è¡Œåˆ—
    :param W_V: Valueã®é‡ã¿è¡Œåˆ—
    :return: Q, K, Vè¡Œåˆ—
    """
    # Queryè¡Œåˆ—ã‚’è¨ˆç®—
    Q = np.dot(embedding_matrix, W_Q)  # ãƒˆãƒ¼ã‚¯ãƒ³æ•° Ã— åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
    # Keyè¡Œåˆ—ã‚’è¨ˆç®—
    K = np.dot(embedding_matrix, W_K)  # ãƒˆãƒ¼ã‚¯ãƒ³æ•° Ã— åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
    # Valueè¡Œåˆ—ã‚’è¨ˆç®—
    V = np.dot(embedding_matrix, W_V)  # ãƒˆãƒ¼ã‚¯ãƒ³æ•° Ã— åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
    return Q, K, V

def Single_Head_Attention(Q, K, V, embedding_dim):
    """
    Scaled Dot-Product Attentionã®è¨ˆç®—
    :param Q: Queryè¡Œåˆ—(å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•° Ã— åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒæ•°ã®å½¢çŠ¶ã‚’æŒã¤è¡Œåˆ—)
    :param K: Keyè¡Œåˆ—
    :param V: Valueè¡Œåˆ—
    :return: Attentionã«ã‚ˆã‚‹å‡ºåŠ›
    """
    #åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ(ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼)
    embedding_dim = embedding_dim

    """
    ãƒ‰ãƒƒãƒˆç© QKt: Queryã¨Keyã®é¡ä¼¼æ€§ã‚’è¨ˆç®—ã—ã€å„ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é–¢é€£åº¦ã‚’å¾—ã‚‹ã€‚
    Queryã¨Keyã®è»¢ç½®è¡Œåˆ—ã®ãƒ‰ãƒƒãƒˆç©ã‚’è¨ˆç®—ã—ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    Qã®iè¡Œç›®ã¯å…¥åŠ›ã®iç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³(1è¡Œ768åˆ—ã®è¡Œåˆ—)â‡’Kã¨ã®ãƒ‰ãƒƒãƒˆç©ã§ä¸€ã¤ã®å€¤ãŒå‡ºã‚‹
    scoresãŒæŒã¤ã®ã¯å…¥åŠ›é•·Ã—å…¥åŠ›é•·ã‚µã‚¤ã‚ºã‚’æŒã¤è¡Œåˆ—ã¨ãªã‚‹
    """
    scores = np.dot(Q, K.T) / np.sqrt(embedding_dim)
    #scores = np.clip(scores, -500, 500)  # ã‚¹ã‚³ã‚¢ã‚’é©åº¦ã«ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°

    """
    ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚’é©ç”¨ã—ã¦Attentionã®é‡ã¿ã‚’è¨ˆç®—
    å…¥åŠ›é•·Ã—å…¥åŠ›é•·ã‚µã‚¤ã‚ºã®ç¢ºç‡å€¤è¡Œåˆ—ãŒå¾—ã‚‰ã‚Œã‚‹
    ã“ã®ç¢ºç‡å€¤è¡Œåˆ—ã«ãŠã„ã¦iè¡Œjåˆ—ã®å€¤ã¯ã€
    å…¥åŠ›ä¸Šã®jç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒå…¥åŠ›ä¸Šã®iç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦ã©ã‚Œã ã‘æ³¨æ„ã‚’å‘ã‘ã‚‹ã‹ã‚’ç¤ºã™
    """
    attention_weights = softmax(scores)
    #attention_weights = np.clip(attention_weights, 1e-5, 1) # æ¥µç«¯ãªå°ã•ãªå€¤ã‚’é˜²ã
    """
    Attentioné‡ã¿ã¨Valueè¡Œåˆ—ã®ãƒ‰ãƒƒãƒˆç©ã‚’è¨ˆç®—ã—ã¦æœ€çµ‚çš„ãªå‡ºåŠ›ã‚’å¾—ã‚‹
    Vã¯å…¥åŠ›é•·Ã—åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã‚µã‚¤ã‚ºã®è¡Œåˆ—ã§å„ãƒˆãƒ¼ã‚¯ãƒ³ã®ã€Œæ„å‘³ã€ã‚’æŒã¡ã€
    Attentioné‡ã¿ã¯å…¥åŠ›é•·Ã—å…¥åŠ›é•·ã‚µã‚¤ã‚ºã®è¡Œåˆ—ã§ãã‚Œãã‚Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã™ã‚‹æ³¨æ„é‡ã‚’æŒã¤
    ãƒ‰ãƒƒãƒˆç©ã§Vã®1è¡Œã¨Attentioné‡ã¿ã®1åˆ—ã®é‡ã¿ä»˜ãå’ŒãŒå¾—ã‚‰ã‚Œã‚‹
    ã—ãŸãŒã£ã¦ã€ãƒ‰ãƒƒãƒˆç©ã®çµæœ1è¡ŒÃ—åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã‚µã‚¤ã‚ºã®è¡Œåˆ—ãŒå„ãƒˆãƒ¼ã‚¯ãƒ³ã«ã¤ã„ã¦å¾—ã‚‰ã‚Œã‚‹(å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°Ã—åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°è¡Œåˆ—)
    """
    output = np.dot(attention_weights, V)

    return output, attention_weights

def Multi_Head_Attention(Q, K, V, embedding_dim, num_heads):
    """
    MHAã¨ã¯åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒÃ·ãƒ˜ãƒƒãƒ‰æ•°ã®æ¬¡å…ƒã‚’ã‚‚ã£ã¦å…¥åŠ›ã‚’å¤šè§’çš„ã«è§£é‡ˆã—ã€
    ãã‚Œã‚’æœ€å¾Œã«çµåˆã™ã‚‹ã“ã¨ã§å…ƒã®å…¥åŠ›ã¨åŒã˜ã‚µã‚¤ã‚ºã®è¡Œåˆ—ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨ã§ã€
    ãƒ˜ãƒƒãƒ‰æ•°ãŒ8ã§ã‚ã‚Œã°8å€‹åˆ†ã®è§£é‡ˆã‚’æŒã£ãŸåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒãŒä½œæˆã§ãã‚‹
    """
    #åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ(ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼)
    embedding_dim = 768
    #MHAã®ãƒ˜ãƒƒãƒ‰æ•°
    num_heads = num_heads
    """
    ãƒ˜ãƒƒãƒ‰æ•°ã§åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’å‰²ã‚‹ç†ç”±ï¼š
    1ï¼è¨ˆç®—ã®åŠ¹ç‡åŒ–ï¼šå„ãƒ˜ãƒƒãƒ‰ã”ã¨ã®è¨ˆç®—é‡ã‚’æ¸›ã‚‰ã™ã“ã¨ãŒå¯èƒ½
    2ï¼åˆ†æ•£ã—ãŸä¸¦åˆ—å‡¦ç†ã®å®Ÿç¾ï¼šå„ãƒ˜ãƒƒãƒ‰ã”ã¨ã«ç•°ãªã‚‹è¦–ç‚¹ã‹ã‚‰å…¥åŠ›ã‚’è§£é‡ˆã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šç²¾åº¦ã®é«˜ã„æ¨è«–ã‚’å®Ÿç¾
    """
    #å„ãƒ˜ãƒƒãƒ‰ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
    d_k = embedding_dim // num_heads

    #å„ãƒ˜ãƒƒãƒ‰ã§å¾—ã‚‰ã‚ŒãŸé‡ã¿è¡Œåˆ—ã¨æ³¨æ„è¡Œåˆ—ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ
    heads_output = []
    heads_weights = []

    for i in range(num_heads):
        # Query, Key, Valueã®é‡ã¿è¡Œåˆ—ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ï¼‰
        W_Q = np.random.randn(embedding_dim, d_k) * np.sqrt(2 / embedding_dim)
        W_K = np.random.randn(embedding_dim, d_k) * np.sqrt(2 / embedding_dim)
        W_V = np.random.randn(embedding_dim, d_k) * np.sqrt(2 / embedding_dim)

        Q_head = np.dot(Q, W_Q)
        K_head = np.dot(K, W_K)
        V_head = np.dot(V, W_V)

        # 3. Single-Head Attentionã‚’å®Ÿè¡Œ
        head_output, head_weights = Single_Head_Attention(Q_head, K_head, V_head, d_k)

        heads_output.append(head_output)
        heads_weights.append(head_weights)

    # 4. Concatenate heads
    concatenated_heads = np.concatenate(heads_output, axis=-1)

    # 5. æœ€çµ‚çš„ãªç·šå½¢å¤‰æ›ã‚’å®Ÿè¡Œï¼ˆãƒ˜ãƒƒãƒ‰ã®å‡ºåŠ›ã‚’çµ±åˆï¼‰
    W_O = np.random.randn(concatenated_heads.shape[-1], embedding_dim)
    final_output = np.dot(concatenated_heads, W_O)

    return final_output, heads_weights

def relu(x):
    return np.maximum(0, x)

def forward(W1, W2, x, b1, b2):
    """
    ä¸€åº¦ã¶ã‚ã£ã¨åºƒã’ãŸè¡Œåˆ—ã«å…¥åŠ›ã®å€¤ã‚’ãƒãƒ©ã¾ã„ã¦è¨ˆç®—ã—ã¦ã€
    ReLUã§0ä»¥ä¸‹ã‚’æ¶ˆã—ã€å¾—ãŸçµæœã®è¡Œåˆ—ã‚’ã‚‚ã¨ã®æ¬¡å…ƒã¨åŒã˜è¡Œåˆ—ã«å†åº¦ãƒãƒ©ã¾ã„ã¦è¨ˆç®—ã™ã‚‹
    çµæœä¸€åº¦è§£é‡ˆç¯„å›²ã‚’åºƒã’ãŸå¾Œã«å…ƒã®ã‚µã‚¤ã‚ºã«åœ§ç¸®ã•ã‚ŒãŸè¡Œåˆ—ãŒå¾—ã‚‰ã‚Œã‚‹
    """
    # ç¬¬1ã®å…¨çµåˆå±¤ã¨ReLUæ´»æ€§åŒ–é–¢æ•°
    x = np.dot(x, W1) + b1
    x = relu(x)
    
    # ç¬¬2ã®å…¨çµåˆå±¤
    x = np.dot(x, W2) + b2
    return x

def feedforward(embedding_dim, x):
    """
    ã“ã“ã§ã¯2å±¤ã®é †ä¼æ’­NNã¨ã—ã¦å®šç¾©ã™ã‚‹
    """
    #éš ã‚Œæ¬¡å…ƒå±¤(ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
    hidden_dim = 2048    # éš ã‚Œå±¤ã®æ¬¡å…ƒæ•°

    W1 = np.random.randn(embedding_dim, hidden_dim) * np.sqrt(2 / embedding_dim)
    W2 = np.random.randn(hidden_dim, embedding_dim) * np.sqrt(2 / hidden_dim)
    b1 = np.zeros((1, hidden_dim))
    b2 = np.zeros((1, embedding_dim))

    output = forward(W1, W2, x, b1, b2)

    return output

def Layer_Normalization(x):
    """
    ãƒ‡ãƒ¼ã‚¿ã®å„è¡Œï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®ç‰¹å¾´é‡ãŒã€å¹³å‡0ã€æ¨™æº–åå·®1ã®æ­£è¦åŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã•ã‚Œã¾ã™.
    ã“ã‚Œã«ã‚ˆã‚Šã€ç•°ãªã‚‹å±¤ã§ã®åˆ†å¸ƒã®å¤‰åŒ–ã«ä¾å­˜ã›ãšã€å®‰å®šã—ãŸå­¦ç¿’ãŒå¯èƒ½ã«ãªã‚Šã¾ã™.
    """
    epsilon=1e-6
    """Layer Normalization"""
    # å„è¡Œã«å¯¾ã™ã‚‹å¹³å‡ã‚’è¨ˆç®— (axis=-1ã¯è¡Œæ–¹å‘ã§è¨ˆç®—)
    mean = np.mean(x, axis=-1, keepdims=True)
    
    # å„è¡Œã«å¯¾ã™ã‚‹æ¨™æº–åå·®ã‚’è¨ˆç®— (axis=-1ã¯è¡Œæ–¹å‘ã§è¨ˆç®—)
    std = np.std(x, axis=-1, keepdims=True)
    
    # å…¥åŠ›xã‚’æ­£è¦åŒ– (mean=0, std=1ã«ãªã‚‹ã‚ˆã†ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°)
    return (x - mean) / (std + epsilon)

def residual_connection(x, sublayer_output):
    """
    æ®‹å·®çµåˆ: sublayer_output (MHA ã¾ãŸã¯ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®å‡ºåŠ›) ã«å…ƒã®å…¥åŠ› x ã‚’è¶³ã—åˆã‚ã›ã‚‹ã€‚
    ã•ã‚‰ã«ã€è¶³ã—åˆã‚ã›ãŸã‚‚ã®ã«Layer Normalizationã‚’é©ç”¨ã€‚
    """
    return Layer_Normalization(x + sublayer_output)

def dropout(x):
    """
    X: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
    drop_prob: ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ï¼ˆä¾‹: 0.5ï¼‰
    """
    #ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡(ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
    drop_prob = 0.5

    """
    å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã‚’Pythonï¼šã‚¢ãƒ³ãƒ‘ãƒƒã‚¯æ©Ÿèƒ½ã§æ¸¡ã—ã¦ä¸€æ§˜ä¹±æ•°åˆ†å¸ƒã‚’ä½œæˆ
    drop_probã®ç¢ºç‡ã§å€¤ã‚’0ã«ã™ã‚‹
    """
    dropout_mask = np.random.rand(*x.shape) > drop_prob

    #å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«ãƒã‚¹ã‚¯ã‚’é©ç”¨ã—ã¦ä¸€éƒ¨ã®å€¤ã‚’ç„¡åŠ¹åŒ–
    x_dropped = x*dropout_mask

    #ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ã«å¿œã˜ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
    #ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ã«å¿œã˜ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€å…¨ã¦ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¨æœŸå¾…å€¤ãŒä¸€è‡´ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹
    x_scaled = x_dropped / (1.0 - drop_prob)

    return x_scaled

#æ­£è¦ç‰ˆé–¢æ•°å®šç¾©
def MHA(Q, K, V, embedding_dim, num_heads):
    Multi_Head_Attention(Q, K, V, embedding_dim, num_heads)

if __name__ == "__main__":
    # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã¨åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã®ä¾‹
    E_len = 100  # å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
    embedding_dim = 384  # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒ

    # ãƒ‡ãƒ¼ã‚¿ã®ä¾‹ã‚’è¡¨ç¤º
    #for i in range(10):
        #print(train_dataset[i])
        #print(valid_dataset[i])
        #print(test_dataset[i])

    #tokenizerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ&ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
    #tokenizer()
    tokenizer = loadTokenizer()
    #useTokenizer(tokenizer)
    embedding_matrix = createTokenEmbeddingMatrix(tokenizer)
    #print(embedding_matrix)
    positional_matrix = createPositionalEmbeddingMatrix(tokenizer)
    #print(positional_matrix)
    embedding_matrix = combineTokenAndPositional(embedding_matrix, positional_matrix)
    #print(embedding_matrix)
    #useTokenEmbeddingMatrix(embedding_matrix)

    # Query, Key, Valueã®é‡ã¿è¡Œåˆ—ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ï¼‰
    #W_Q = np.random.randn(embedding_dim, embedding_dim) * np.sqrt(2 / embedding_dim)
    #W_K = np.random.randn(embedding_dim, embedding_dim) * np.sqrt(2 / embedding_dim)
    #W_V = np.random.randn(embedding_dim, embedding_dim) * np.sqrt(2 / embedding_dim)
    # Q, K, Vã®è¨ˆç®—
    #Q, K, V = calculate_QKV(embedding_matrix, W_Q, W_K, W_V)
    #output, attention_weights = Multi_Head_Attention(Q, K, V, embedding_dim, 8)
    #print(output)
    #print(attention_weights)

    # ãƒ€ãƒŸãƒ¼å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ (1ã¤ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã™ã‚‹åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«)
    x = np.random.randn(1, embedding_dim)  # 1ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«
    output = feedforward(embedding_dim, x) #æœ¬æ¥ã¯MHAã®outputã‚’xã«æ¸¡ã™
    print(output)

    #æ®‹å·®çµåˆã¨å±¤æ­£è¦åŒ–
    output_from_mha = np.random.randn(1, embedding_dim)  # MHAã‚„ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ã®å‡ºåŠ›
    x_input = np.random.randn(1, embedding_dim)  # å…ƒã®å…¥åŠ›

    # æ®‹å·®çµåˆ
    output_with_residual = residual_connection(x_input, output_from_mha)

    print(output_with_residual)

    #Dropout
    # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿
    X = np.array([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])

    # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’é©ç”¨
    X_after_dropout = dropout(X)

    print("ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆå¾Œã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿:")
    print(X_after_dropout)

    #æ­£è¦ç‰ˆ
