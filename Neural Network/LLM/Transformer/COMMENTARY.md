# 分類
このフォルダではLLMの前提となるTransformerアーキテクチャについてPytorchで実装してみる。

# Transformerとは

# 関数説明
### `TransformerLanguageModel`
モデルの動作を定義するクラス

### `forward`
`x`は入力テンソル。
もちろんです。`TransformerLanguageModel` の `forward` 関数で扱われる入力テンソルには、次のような情報が含まれています。

### 入力テンソル `x` の構造
`x` は `forward` 関数に渡されるモデルの入力テンソルです。このテンソルにはトークン化された文章の系列情報が格納されています。

- **形状**: `[batch_size, seq_length]`
  - `batch_size`: ミニバッチ内のサンプルの数（通常はモデルの効率的な学習のために複数のサンプルを一度に処理します）。
  - `seq_length`: それぞれの入力シーケンス（文章）の長さを示します。この長さは、トークン化後のトークン数で表されます。

### **具体的な内容と値についての説明**
入力テンソル `x` のそれぞれの値は、語彙内の単語（またはトークン）を示す整数です。この整数は、単語ごとにユニークなインデックスを持つ語彙（ボキャブラリ）に対応しています。

たとえば、次のように考えます。

- **語彙サイズ `vocab_size = 10000`**: 語彙には10,000個の単語（トークン）があり、それぞれに0から9999までのインデックスが割り当てられています。
- **入力テンソル `x` の各要素**:
  - 各要素（インデックス）は、特定の単語を語彙から参照するためのものです。たとえば、インデックスが `5` の場合、語彙の中で `5番目` に対応する単語がその位置に入っています。

### **入力テンソルの例**
例えば、以下のようなミニバッチがあったとします：

```python
x = [[4, 15, 6, 20],
     [7, 9, 0, 0]]
```

この場合、

- **`batch_size = 2`**（2つのシーケンスが含まれています）。
- **`seq_length = 4`**（各シーケンスの長さは4トークンです）。

### それぞれのシーケンス
1つ目のシーケンス `[4, 15, 6, 20]`:
このシーケンスは4つのトークンからなり、それぞれのトークンは語彙の中で特定の単語を示します（例えば、インデックス `4` は語彙中の5番目の単語に対応します）。
2つ目のシーケンス `[7, 9, 0, 0]`:
こちらのシーケンスも同様で、後半の `0` はおそらくパディング（シーケンスの長さを統一するために追加される特別なトークン）を示しています。

### **入力テンソルの処理について**
1. **`token_embedding` 層**: 
   - `x` に対して、トークンの埋め込みを行います。
   - 各トークン（整数）を `embed_size` のベクトルに変換します。この結果、`x` の形状は `[batch_size, seq_length, embed_size]` になります。

2. **`position_embedding` 層**:
   - 各トークンの位置を考慮するため、位置埋め込みを追加します。位置埋め込みは、シーケンス内の各トークンの位置情報を持つベクトルを付加します。
   - 位置情報も `[batch_size, seq_length, embed_size]` の形状を持ちます。

3. **埋め込みの加算**:
   - トークン埋め込みと位置埋め込みを足し合わせることで、モデルがトークンの内容とその位置情報の両方を考慮できるようになります。

### **まとめ**
- **`x` の各値**は、語彙中の単語のインデックスを示す整数です。
- **埋め込み層で処理されるとき**、これらのインデックスが意味のある連続ベクトルに変換され、モデルが理解しやすい形式でトークン情報を表現します。
- Transformerはこれらの埋め込みを処理し、系列全体の関係を学習して次のトークンの予測を行います。

これが入力テンソル `x` の構造と、それぞれの値が意味する内容です。理解の助けになれば幸いです。他に質問があれば教えてください。